{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1598359252033",
   "display_name": "Python 3.8.3 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datasets:\n",
    "\n",
    "    https://www.kaggle.com/bittlingmayer/amazonreviews\n",
    "    http://deepyeti.ucsd.edu/jianmo/amazon/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "# Reading the json dataset into a pandas dataframe\n",
    "df = pd.read_json('E:/Internships/TCS-iON/Code/MyCode/Amazon/Prime_Pantry.json', orient = 'records', lines = True) \n",
    "# Adding a column representing 1 for 'pos' and 0 for 'neg' sentiments\n",
    "df['senti'] = df.apply(lambda x: 1 if x['overall'] >= 4 else 0, axis = 1) \n",
    "# Deleting unnecessary columns\n",
    "df = df.drop(['verified', 'reviewTime', 'asin', 'reviewerName', 'summary', 'unixReviewTime', 'vote', 'image', 'style', 'reviewerID'], axis = 1) \n",
    "# Converting the data type to string \n",
    "df['reviewText'] = df[\"reviewText\"].astype(\"str\")\n",
    "# Converting all text to lowercase for use\n",
    "df['reviewText'] = df['reviewText'].str.lower()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table border = '1'>\n",
    "<tr align = 'center'>\n",
    "<td>overall</td><td></td><td>review</td><td></td>\t<td>senti</td>\n",
    "<hr>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>5</td><td></td><td>good clinging</td><td></td>\t<td>1</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>4</td><td></td><td>fantastic buy and a good plastic wrap. even t...</td><td></td>\t<td>1</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>4</td><td></td><td>ok</td><td></td>\t\t<td>1</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>3</td><td></td><td>saran cling plus is kind of like most of the c...</td><td></td>\t\t<td>0</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>4</td><td></td><td>this is my go to plastic wrap so there isn't m...</td><td></td>\t<td>1</td>\n",
    "</tr>\n",
    "</table>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "saran cling plus is kind of like most of the cling wrap from glad. it is a very good quality plastic wrap, but the delivery system is poorly executed, and this makes usage more frustrating and less time-efficient. as convenience is one of the core selling points of this sort of product, how easy it is to use is just as important as how good the wrap itself is.\n",
    "\n",
    "as another user here noted, getting this stuff out of the box and tearing off the portion you need to use is very difficult. substantial force is required to do this due to a cutting system that is lacking, and this can result in the box being damaged as it is not a very robust box. it can also cause the piece you are attempting to tear off fold up from the pressure upon being torn, leaving you with a tangled mess to untangle. as the blade itself does not do a great job cutting, sometimes the wrap will tear, leaving you with a piece that is a different size than you wanted. the physical location of where the wrap comes out relative to where it is cut at also results in one having to hold at an awkward angle to see what they are doing, making all of this more difficult.\n",
    "\n",
    "i use clear wrap multiple times a day...the difference between a delivery system that is easy versus difficult means a lot of time savings, and a lot of frustration that does not have to occur. this is a good quality wrap, but like glad's wrap, it's delivery system is lacking compared to other products (such as the kirkland clear wraps) and so the convenience factor is reduced. so i think that you will find that an alternative clear wrap product with a better system for dispensing & cutting is more enjoyable to use, and much more convenient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from nltk import WordNetLemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "# Initialising the nltk stop_words, stemmer and lemmatizer functions\n",
    "stop_words = set(stopwords.words(\"english\")) \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "# Creating a function for text cleaning\n",
    "def textCleanser(myText):\n",
    "    # Removing the name titles and the period symbols after it\n",
    "    myText = re.sub(r'[mdsr]r(s)?\\.', '', myText)\n",
    "    # Removing punctuation\n",
    "    myPunct = string.punctuation\n",
    "    punctToSpace = str.maketrans(myPunct, len(myPunct)*' ')   \n",
    "    myText = myText.translate(punctToSpace) \n",
    "    # Removing the '@username' mentions\n",
    "    myText = re.sub(r'@\\w+', '', myText)\n",
    "    # Removing urls\n",
    "    myText = re.sub(r'(http(s)?://)?(www\\.)?.+\\.com', '', myText)\n",
    "    # Removing numbers\n",
    "    myText = re.sub(r'\\d+', '', myText)    \n",
    "    # Removing stopwords\n",
    "    myText = [word for word in myText.split(' ') if not word in stop_words]\n",
    "    myText = [word for word in myText if word != '']\n",
    "    # Lemmatizing the text\n",
    "    myText = [lemmatizer.lemmatize(token) for token in myText]\n",
    "    # Stemming the text\n",
    "    # myText = [stemmer.stem(token) for token in myText]\n",
    "    return myText\n",
    "for i in range(len(df['reviewText'])):\n",
    "    df['reviewText'][i] = textCleanser(df['reviewText'][i])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table border = '1'>\n",
    "<tr align = 'center'>\n",
    "<td>overall</td><td></td><td>review</td><td></td>\t<td>senti</td>\n",
    "<hr>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>5</td><td></td><td>[good, clinging]</td><td></td>\t<td>1</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>4</td><td></td><td>[fantastic, buy, good, plastic, wrap, even, th...</td><td></td>\t<td>1</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>4</td><td></td><td>[ok]</td><td></td>\t\t<td>1</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>3</td><td></td><td>\t[saran, cling, plus, kind, like, cling, wrap, ...</td><td></td>\t\t<td>0</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>4</td><td></td><td>[go, plastic, wrap, much, bad, say, plastic, w...</td><td></td>\t<td>1</td>\n",
    "</tr>\n",
    "</table>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'[saran', 'cling', 'plus', 'kind', 'like', 'cling', 'wrap', 'glad', 'good', 'quality', 'plastic', 'wrap', 'delivery', 'system', 'poorly', 'executed', 'make', 'usage', 'frustrating', 'le', 'time', 'efficient', 'convenience', 'one', 'core', 'selling', 'point', 'sort', 'product', 'easy', 'use', 'important', 'good', 'wrap', '\\n\\nas', 'another', 'user', 'noted', 'getting', 'stuff', 'box', 'tearing', 'portion', 'need', 'use', 'difficult', 'substantial', 'force', 'required', 'due', 'cutting', 'system', 'lacking', 'result', 'box', 'damaged', 'robust', 'box', 'also', 'cause', 'piece', 'attempting', 'tear', 'fold', 'pressure', 'upon', 'torn', 'leaving', 'tangled', 'mess', 'untangle', 'blade', 'great', 'job', 'cutting', 'sometimes', 'wrap', 'tear', 'leaving', 'piece', 'different', 'size', 'wanted', 'physical', 'location', 'wrap', 'come', 'relative', 'cut', 'also', 'result', 'one', 'hold', 'awkward', 'angle', 'see', 'making', 'difficult', '\\n\\ni', 'use', 'clear', 'wrap', 'multiple', 'time', 'day', 'difference', 'delivery', 'system', 'easy', 'versus', 'difficult', 'mean', 'lot', 'time', 'saving', 'lot', 'frustration', 'occur', 'good', 'quality', 'wrap', 'like', 'glad', 'wrap', 'delivery', 'system', 'lacking', 'compared', 'product', 'kirkland', 'clear', 'wrap', 'convenience', 'factor', 'reduced', 'think', 'find', 'alternative', 'clear', 'wrap', 'product', 'better', 'system', 'dispensing', 'cutting', 'enjoyable', 'use', 'much', 'convenient']''\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myReviews = []\n",
    "for i in range(len(df['reviewText'])):\n",
    "    for j in df['reviewText'][i]:\n",
    "        if j != 'br' and j != 'http':\n",
    "            myReviews.append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import collections\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np \n",
    "np.random.seed(1234)\n",
    "# Initialising the Count Vectorizer\n",
    "cv = CountVectorizer()\n",
    "myBow = cv.fit_transform(myReviews)\n",
    "wordFrequency = dict(zip(cv.get_feature_names(), np.asarray(myBow.sum(axis = 0)).ravel()))\n",
    "wordCounter = collections.Counter(wordFrequency)\n",
    "# Storing the frequency of appearance of words\n",
    "dfWordCounter = pd.DataFrame(wordCounter.most_common(25), columns = ['word', 'frequency'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the top 25 most frequently occuring words\n",
    "plt.close('all')\n",
    "fig, ax = plt.subplots(figsize = (17, 12))\n",
    "sns.barplot(x = 'word', y = 'frequency', data = dfWordCounter, ax = ax)\n",
    "sns.set_palette('pastel') \n",
    "plt.xlabel('Words')\n",
    "plt.ylabel('Frequency of appearance of words')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"top25words.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense , Input , LSTM , Embedding, Dropout , Activation, GRU, Flatten\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Convolution1D\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "# Splitting the dataframe to training and testing data\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[\"review\"], df['senti'],test_size=0.2, random_state=42)\n",
    "# Initialising the tokenizer\n",
    "max_features = 5000\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(X_train)\n",
    "maxlen = 130\n",
    "X_t = pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "y = y_train\n",
    "embed_size = 128\n",
    "# Initializing a bidirectional sequential LSTM using Adam optimizer, positive activation function(relu), and between 0 and 1 using sigmoid\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, embed_size))\n",
    "model.add(Bidirectional(LSTM(32, return_sequences = True)))\n",
    "model.add(GlobalMaxPool1D())\n",
    "model.add(Dense(20, activation=\"relu\"))\n",
    "model.add(Dropout(0.05))\n",
    "model.add(Dense(1, activation=\"sigmoid\"))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "batch_size = 100\n",
    "epochs = 5\n",
    "model.fit(X_t,y, batch_size=batch_size, epochs=epochs, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the model\n",
    "list_sentences_test = X_test\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\n",
    "X_te = pad_sequences(list_tokenized_test, maxlen=maxlen)\n",
    "prediction = model.predict(X_te)\n",
    "y_pred = (prediction > 0.5)\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "print('F1-score: {0}'.format(f1_score(y_pred, y_test)))\n",
    "print('Confusion matrix:')\n",
    "confusion_matrix(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "F1-score: \n",
    "\n",
    "       0.9429220134457394\n",
    "\n",
    "Confusion matrix:\n",
    "\n",
    "       [[10752, 3162 ],\n",
    "        [ 5863, 74546]]\n",
    "\n",
    "Accuracy:\n",
    "\n",
    "       90.43%"
   ]
  }
 ]
}