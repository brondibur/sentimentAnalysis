{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1598683770214",
   "display_name": "Python 3.8.3 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datasets:\n",
    "\n",
    "    https://www.kaggle.com/bittlingmayer/amazonreviews\n",
    "    http://deepyeti.ucsd.edu/jianmo/amazon/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "# Reading the json dataset into a pandas dataframe\n",
    "df = pd.read_json('E:/Internships/TCS-iON/Code/MyCode/Amazon/Prime_Pantry.json', orient = 'records', lines = True) \n",
    "# Adding a column representing 1 for 'pos' and 0 for 'neg' sentiments\n",
    "df['senti'] = df.apply(lambda x: 1 if x['overall'] >= 4 else 0, axis = 1) \n",
    "# Deleting unnecessary columns\n",
    "df = df.drop(['verified', 'reviewTime', 'asin', 'reviewerName', 'summary', 'unixReviewTime', 'vote', 'image', 'style', 'reviewerID'], axis = 1) \n",
    "# Converting the data type to string \n",
    "df['reviewText'] = df[\"reviewText\"].astype(\"str\")\n",
    "# Converting all text to lowercase for use\n",
    "df['reviewText'] = df['reviewText'].str.lower()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table border = '1'>\n",
    "<tr align = 'center'>\n",
    "<td>overall</td><td></td><td>review</td><td></td>\t<td>senti</td>\n",
    "<hr>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>5</td><td></td><td>good clinging</td><td></td>\t<td>1</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>4</td><td></td><td>fantastic buy and a good plastic wrap. even t...</td><td></td>\t<td>1</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>4</td><td></td><td>ok</td><td></td>\t\t<td>1</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>3</td><td></td><td>saran cling plus is kind of like most of the c...</td><td></td>\t\t<td>0</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>4</td><td></td><td>this is my go to plastic wrap so there isn't m...</td><td></td>\t<td>1</td>\n",
    "</tr>\n",
    "</table>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "saran cling plus is kind of like most of the cling wrap from glad. it is a very good quality plastic wrap, but the delivery system is poorly executed, and this makes usage more frustrating and less time-efficient. as convenience is one of the core selling points of this sort of product, how easy it is to use is just as important as how good the wrap itself is.\n",
    "\n",
    "as another user here noted, getting this stuff out of the box and tearing off the portion you need to use is very difficult. substantial force is required to do this due to a cutting system that is lacking, and this can result in the box being damaged as it is not a very robust box. it can also cause the piece you are attempting to tear off fold up from the pressure upon being torn, leaving you with a tangled mess to untangle. as the blade itself does not do a great job cutting, sometimes the wrap will tear, leaving you with a piece that is a different size than you wanted. the physical location of where the wrap comes out relative to where it is cut at also results in one having to hold at an awkward angle to see what they are doing, making all of this more difficult.\n",
    "\n",
    "i use clear wrap multiple times a day...the difference between a delivery system that is easy versus difficult means a lot of time savings, and a lot of frustration that does not have to occur. this is a good quality wrap, but like glad's wrap, it's delivery system is lacking compared to other products (such as the kirkland clear wraps) and so the convenience factor is reduced. so i think that you will find that an alternative clear wrap product with a better system for dispensing & cutting is more enjoyable to use, and much more convenient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from nltk import WordNetLemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "# Initialising the nltk stop_words, stemmer and lemmatizer functions\n",
    "stop_words = set(stopwords.words(\"english\")) \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "# Creating a function for text cleaning\n",
    "def textCleanser(myText):\n",
    "    # Removing the name titles and the period symbols after it\n",
    "    myText = re.sub(r'[mdsr]r(s)?\\.', '', myText)\n",
    "    # Removing punctuation\n",
    "    myPunct = string.punctuation\n",
    "    punctToSpace = str.maketrans(myPunct, len(myPunct)*' ')   \n",
    "    myText = myText.translate(punctToSpace) \n",
    "    # Removing the '@username' mentions\n",
    "    myText = re.sub(r'@\\w+', '', myText)\n",
    "    # Removing urls\n",
    "    myText = re.sub(r'(http(s)?://)?(www\\.)?.+\\.com', '', myText)\n",
    "    # Removing numbers\n",
    "    myText = re.sub(r'\\d+', '', myText)    \n",
    "    # Removing stopwords\n",
    "    myText = [word for word in myText.split(' ') if not word in stop_words]\n",
    "    myText = [word for word in myText if word != '']\n",
    "    # Lemmatizing the text\n",
    "    myText = [lemmatizer.lemmatize(token) for token in myText]\n",
    "    # Stemming the text\n",
    "    # myText = [stemmer.stem(token) for token in myText]\n",
    "    return myText\n",
    "for i in range(len(df['reviewText'])):\n",
    "    df['reviewText'][i] = textCleanser(df['reviewText'][i])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table border = '1'>\n",
    "<tr align = 'center'>\n",
    "<td>overall</td><td></td><td>review</td><td></td>\t<td>senti</td>\n",
    "<hr>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>5</td><td></td><td>[good, clinging]</td><td></td>\t<td>1</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>4</td><td></td><td>[fantastic, buy, good, plastic, wrap, even, th...</td><td></td>\t<td>1</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>4</td><td></td><td>[ok]</td><td></td>\t\t<td>1</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>3</td><td></td><td>\t[saran, cling, plus, kind, like, cling, wrap, ...</td><td></td>\t\t<td>0</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>4</td><td></td><td>[go, plastic, wrap, much, bad, say, plastic, w...</td><td></td>\t<td>1</td>\n",
    "</tr>\n",
    "</table>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'[saran', 'cling', 'plus', 'kind', 'like', 'cling', 'wrap', 'glad', 'good', 'quality', 'plastic', 'wrap', 'delivery', 'system', 'poorly', 'executed', 'make', 'usage', 'frustrating', 'le', 'time', 'efficient', 'convenience', 'one', 'core', 'selling', 'point', 'sort', 'product', 'easy', 'use', 'important', 'good', 'wrap', '\\n\\nas', 'another', 'user', 'noted', 'getting', 'stuff', 'box', 'tearing', 'portion', 'need', 'use', 'difficult', 'substantial', 'force', 'required', 'due', 'cutting', 'system', 'lacking', 'result', 'box', 'damaged', 'robust', 'box', 'also', 'cause', 'piece', 'attempting', 'tear', 'fold', 'pressure', 'upon', 'torn', 'leaving', 'tangled', 'mess', 'untangle', 'blade', 'great', 'job', 'cutting', 'sometimes', 'wrap', 'tear', 'leaving', 'piece', 'different', 'size', 'wanted', 'physical', 'location', 'wrap', 'come', 'relative', 'cut', 'also', 'result', 'one', 'hold', 'awkward', 'angle', 'see', 'making', 'difficult', '\\n\\ni', 'use', 'clear', 'wrap', 'multiple', 'time', 'day', 'difference', 'delivery', 'system', 'easy', 'versus', 'difficult', 'mean', 'lot', 'time', 'saving', 'lot', 'frustration', 'occur', 'good', 'quality', 'wrap', 'like', 'glad', 'wrap', 'delivery', 'system', 'lacking', 'compared', 'product', 'kirkland', 'clear', 'wrap', 'convenience', 'factor', 'reduced', 'think', 'find', 'alternative', 'clear', 'wrap', 'product', 'better', 'system', 'dispensing', 'cutting', 'enjoyable', 'use', 'much', 'convenient']''\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myReviews = []\n",
    "for i in range(len(df['reviewText'])):\n",
    "    for j in df['reviewText'][i]:\n",
    "        if j != 'br' and j != 'http':\n",
    "            myReviews.append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import collections\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np \n",
    "np.random.seed(1234)\n",
    "# Initialising the Count Vectorizer\n",
    "cv = CountVectorizer()\n",
    "myBow = cv.fit_transform(myReviews)\n",
    "wordFrequency = dict(zip(cv.get_feature_names(), np.asarray(myBow.sum(axis = 0)).ravel()))\n",
    "wordCounter = collections.Counter(wordFrequency)\n",
    "# Storing the frequency of appearance of words\n",
    "dfWordCounter = pd.DataFrame(wordCounter.most_common(25), columns = ['word', 'frequency'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the top 25 most frequently occuring words\n",
    "plt.close('all')\n",
    "fig, ax = plt.subplots(figsize = (17, 12))\n",
    "sns.barplot(x = 'word', y = 'frequency', data = dfWordCounter, ax = ax)\n",
    "sns.set_palette('pastel') \n",
    "plt.xlabel('Words')\n",
    "plt.ylabel('Frequency of appearance of words')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"top25words.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import torch\n",
    "import torchtext\n",
    "from torchtext import data\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as fn\n",
    "SEED = 1234\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "# Creating functions to generate bigrams\n",
    "def myBigrams(x):\n",
    "    n_grams = set(zip(*[x[i:] for i in range(2)]))\n",
    "    for n_gram in n_grams:\n",
    "        x.append(' '.join(n_gram))\n",
    "    return x\n",
    "def dummyfn(doc):\n",
    "    return doc\n",
    "# Generating bigrams\n",
    "TEXT = data.Field(tokenize = 'spacy', preprocessing = myBigrams)\n",
    "LABEL = data.LabelField(dtype = torch.float)\n",
    "# Creating training and testing datasets\n",
    "training, testing = train_test_split(df, test_size=0.2, random_state=42)\n",
    "training, validating = train_test_split(training, test_size=0.2, random_state=42)\n",
    "training.to_csv('training.csv')\n",
    "validating.to_csv('validating.csv')\n",
    "testing.to_csv('testing.csv')\n",
    "fields = [(None, None), (None, None), ('c', TEXT), ('s', LABEL)]\n",
    "train_data, valid_data, test_data = data.TabularDataset.splits(\n",
    "    path = 'E:/Internships/TCS-iON/Code/MyCode/Amazon/',\n",
    "    train = 'training.csv',\n",
    "    validation = 'validating.csv',\n",
    "    test = 'testing.csv',\n",
    "    format = 'csv',\n",
    "    fields = fields,\n",
    "    skip_header = True\n",
    ")\n",
    "MAX_VOCAB_SIZE = 50000\n",
    "# Building the vocabularies\n",
    "TEXT.build_vocab(\n",
    "    train_data, \n",
    "    max_size = MAX_VOCAB_SIZE, \n",
    "    # Using the glove.6B.100d vector\n",
    "    vectors = \"glove.6B.100d\", \n",
    "    unk_init = torch.Tensor.normal_)\n",
    "LABEL.build_vocab(train_data)\n",
    "BATCH_SIZE = 64\n",
    "device = torch.device('cpu')\n",
    "# Creating vocabulary iterators\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data),\n",
    "    sort = False, \n",
    "    batch_size = BATCH_SIZE, \n",
    "    device = device)\n",
    "# Building the training model\n",
    "class myModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, output_dim, pad_idx):\n",
    "        super().__init__()\n",
    "        # Initialising the embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
    "        # Initialising the linear layer\n",
    "        self.fc = nn.Linear(embedding_dim, output_dim)\n",
    "    def forward(self, text):\n",
    "        embedded = self.embedding(text)\n",
    "        embedded = embedded.permute(1, 0, 2)\n",
    "        # Averaging the sentences in 2d \n",
    "        pooled = fn.avg_pool2d(embedded, (embedded.shape[1], 1)).squeeze(1) \n",
    "        return self.fc(pooled)\n",
    "inputDimension = len(TEXT.vocab)\n",
    "embeddingDimension = 100\n",
    "outputDimension = 1\n",
    "padding = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "model = myModel(inputDimension, embeddingDimension, outputDimension, padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Copying the pre-trained vectors to the embedding layer\n",
    "pretrained_embeddings = TEXT.vocab.vectors\n",
    "model.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "# Setting the unknown and padding elements to zero since they are of no use\n",
    "unknown = TEXT.vocab.stoi[TEXT.unk_token]\n",
    "model.embedding.weight.data[unknown] = torch.zeros(embeddingDimension)\n",
    "model.embedding.weight.data[padding] = torch.zeros(embeddingDimension)\n",
    "# Counting the number of trainable parameters\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    The model has 5,000,301 trainable parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch.optim as optim\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "# Defining function to calculate accuracy\n",
    "def binary_accuracy(preds, y):\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float()\n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc\n",
    "# Defining function for traning\n",
    "def train(model, iterator, optimizer, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.train()\n",
    "    for batch in iterator:\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(batch.c).squeeze(1)\n",
    "        loss = criterion(predictions, batch.s)\n",
    "        acc = binary_accuracy(predictions, batch.s)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "# Defining function for testing\n",
    "def evaluate(model, iterator, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            predictions = model(batch.c).squeeze(1)\n",
    "            loss = criterion(predictions, batch.s)\n",
    "            acc = binary_accuracy(predictions, batch.s)\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "# Defining function to calculate time\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Traning the model\n",
    "N_EPOCHS = 5\n",
    "best_valid_loss = float('inf')\n",
    "for epoch in range(N_EPOCHS):\n",
    "    start_time = time.time()\n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "    end_time = time.time()\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'myModelBigram.pt')\n",
    "    print(f'Epoch: {epoch+1:02}')\n",
    "    print(f'\\tEpoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Epoch: 01\n",
    "\t\n",
    "\tEpoch Time: 18m 36s\n",
    "\tTrain Loss: 0.377 | Train Acc: 84.68%\n",
    "\t Val. Loss: 0.309 |  Val. Acc: 87.11%\n",
    "\n",
    "Epoch: 02\n",
    "\t\n",
    "\tEpoch Time: 18m 56s\n",
    "\tTrain Loss: 0.288 | Train Acc: 88.30%\n",
    "\t Val. Loss: 0.295 |  Val. Acc: 88.20%\n",
    "\n",
    "Epoch: 03\n",
    "\t\n",
    "\tEpoch Time: 18m 51s\n",
    "\tTrain Loss: 0.277 | Train Acc: 88.98%\n",
    "\t Val. Loss: 0.295 |  Val. Acc: 88.33%\n",
    "\n",
    "Epoch: 04\n",
    "\t\n",
    "\tEpoch Time: 18m 47s\n",
    "\tTrain Loss: 0.271 | Train Acc: 89.25%\n",
    "\t Val. Loss: 0.293 |  Val. Acc: 88.59%\n",
    "\n",
    "Epoch: 05\n",
    "\t\n",
    "\tEpoch Time: 18m 59s\n",
    "\tTrain Loss: 0.267 | Train Acc: 89.48%\n",
    "\t Val. Loss: 0.294 |  Val. Acc: 88.56%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Testing the model\n",
    "model.load_state_dict(torch.load('myModelBigram.pt'))\n",
    "test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
    "# Calculating test accuracy\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Test Loss: 0.286 \n",
    "    Test Acc: 88.86%"
   ]
  }
 ]
}