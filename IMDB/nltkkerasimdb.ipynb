{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1598434069299",
   "display_name": "Python 3.8.3 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datasets:\n",
    "\n",
    "    https://www.kaggle.com/utathya/imdb-review-dataset\n",
    "    http://ai.stanford.edu/~amaas/data/sentiment/ (better)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "# Reading the csv dataset into a pandas dataframe\n",
    "df = pd.read_csv('E:/Internships/TCS-iON/Code/MyCode/IMDB/imdb_master.csv', encoding = 'ISO-8859-1') \n",
    "# Adding a column representing 1 for 'pos' and 0 for 'neg' sentiments\n",
    "df['senti'] = df.apply(lambda x: 1 if x['label'] == 'pos' else 0, axis = 1) \n",
    "# Deleting unnecessary columns\n",
    "df = df.drop(['Unnamed: 0', 'type', 'file'], axis = 1) \n",
    "# Converting the data type to string \n",
    "df['review'] = df[\"review\"].astype(\"str\")\n",
    "# Converting all text to lowercase for use\n",
    "df['review'] = df['review'].str.lower()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table border = '1'>\n",
    "<tr align = 'center'>\n",
    "<td>review</td><td></td>\t<td>label</td><td></td>\t<td>senti</td>\n",
    "<hr>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>once again mr. costner has dragged out a movie...</td><td></td>\t<td>neg</td><td></td>\t<td>0</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>this is an example of why the majority of acti...</td><td></td>\t<td>neg</td><td></td>\t<td>0</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>first of all i hate those moronic rappers, who...</td><td></td>\t<td>neg</td><td></td>\t<td>0</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>not even the beatles could write songs everyon...</td><td></td>\t<td>neg</td><td></td>\t<td>0</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>brass pictures (movies is not a fitting word f...</td><td></td>\t<td>neg</td><td></td>\t<td>0</td>\n",
    "</tr>\n",
    "</table>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "once again mr. costner has dragged out a movie for far longer than necessary. aside from the terrific sea rescue sequences, of which there are very few i just did not care about any of the characters. most of us have ghosts in the closet, and costner's character are realized early on, and then forgotten until much later, by which time i did not care. the character we should really care about is a very cocky, overconfident ashton kutcher. the problem is he comes off as kid who thinks he's better than anyone else around him and shows no signs of a cluttered closet. his only obstacle appears to be winning over costner. finally when we are well past the half way point of this stinker, costner tells us all about kutcher's ghosts. we are told why kutcher is driven to be the best with no prior inkling or foreshadowing. no magic here, it was all i could do to keep from turning it off an hour in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from nltk import WordNetLemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "# Initialising the nltk stop_words, stemmer and lemmatizer functions\n",
    "stop_words = set(stopwords.words(\"english\")) \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "# Creating a function for text cleaning\n",
    "def textCleanser(myText):\n",
    "    # Removing the name titles and the period symbols after it\n",
    "    myText = re.sub(r'[mdsr]r(s)?\\.', '', myText)\n",
    "    # Removing punctuation\n",
    "    myPunct = string.punctuation\n",
    "    punctToSpace = str.maketrans(myPunct, len(myPunct)*' ')   \n",
    "    myText = myText.translate(punctToSpace) \n",
    "    # Removing the '@username' mentions\n",
    "    myText = re.sub(r'@\\w+', '', myText)\n",
    "    # Removing urls\n",
    "    myText = re.sub(r'(http(s)?://)?(www\\.)?.+\\.com', '', myText)\n",
    "    # Removing numbers\n",
    "    myText = re.sub(r'\\d+', '', myText)    \n",
    "    # Removing stopwords\n",
    "    myText = [word for word in myText.split(' ') if not word in stop_words]\n",
    "    myText = [word for word in myText if word != '']\n",
    "    # Lemmatizing the text\n",
    "    myText = [lemmatizer.lemmatize(token) for token in myText]\n",
    "    # Stemming the text\n",
    "    # myText = [stemmer.stem(token) for token in myText]\n",
    "    return myText\n",
    "for i in range(len(df['review'])):\n",
    "    df['review'][i] = textCleanser(df['review'][i])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table border = '1'>\n",
    "<tr align = 'center'>\n",
    "<td>review</td><td></td>\t<td>label</td><td></td>\t<td>senti</td>\n",
    "<hr>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>[costner, dragged, movie, far, longer, necessa...</td><td></td>\t<td>neg</td><td></td>\t<td>0</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>[example, majority, action, film, generic, bor...</td><td></td>\t<td>neg</td><td></td>\t<td>0</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>[first, hate, moronic, rapper, could, nt, act,...</td><td></td>\t<td>neg</td><td></td>\t<td>0</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>[even, beatles, could, write, song, everyone, ...</td><td></td>\t<td>neg</td><td></td>\t<td>0</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>[brass, picture, movie, fitting, word, really,...</td><td></td>\t<td>neg</td><td></td>\t<td>0</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "['costner', 'dragged', 'movie', 'far', 'longer', 'necessary', 'aside', 'terrific', 'sea', 'rescue', 'sequence', 'care', 'character', 'u', 'ghost', 'closet', 'costner', 'character', 'realized', 'early', 'forgotten', 'much', 'later', 'time', 'care', 'character', 'really', 'care', 'cocky', 'overconfident', 'ashton', 'kutcher', 'problem', 'come', 'kid', 'think', 'better', 'anyone', 'else', 'around', 'show', 'sign', 'cluttered', 'closet', 'obstacle', 'appears', 'winning', 'costner', 'finally', 'well', 'past', 'half', 'way', 'point', 'stinker', 'costner', 'tell', 'u', 'kutcher', 'ghost', 'told', 'kutcher', 'driven', 'best', 'prior', 'inkling', 'foreshadowing', 'magic', 'could', 'keep', 'turning', 'hour']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "myReviews = []\n",
    "for i in range(len(df['review'])):\n",
    "    for j in df['review'][i]:\n",
    "        if j != 'br':\n",
    "            myReviews.append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import collections\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np \n",
    "np.random.seed(1234)\n",
    "# Initialising the Count Vectorizer\n",
    "cv = CountVectorizer()\n",
    "myBow = cv.fit_transform(myReviews)\n",
    "wordFrequency = dict(zip(cv.get_feature_names(), np.asarray(myBow.sum(axis = 0)).ravel()))\n",
    "wordCounter = collections.Counter(wordFrequency)\n",
    "# Storing the frequency of appearance of words\n",
    "dfWordCounter = pd.DataFrame(wordCounter.most_common(25), columns = ['word', 'frequency'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the top 25 most frequently occuring words\n",
    "plt.close('all')\n",
    "fig, ax = plt.subplots(figsize = (17, 15))\n",
    "sns.barplot(x = 'word', y = 'frequency', data = dfWordCounter, ax = ax)\n",
    "sns.set_palette('pastel') \n",
    "plt.xlabel('Words')\n",
    "plt.ylabel('Frequency of appearance of words')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"top25words.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense , Input , LSTM , Embedding, Dropout , Activation, GRU, Flatten\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Convolution1D\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "# Splitting the dataframe to training and testing data\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[\"review\"], df['senti'],test_size=0.2, random_state=42)\n",
    "# Initialising the tokenizer\n",
    "max_features = 5000\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(X_train)\n",
    "maxlen = 130\n",
    "X_t = pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "y = y_train\n",
    "embed_size = 128\n",
    "# Initializing a bidirectional sequential LSTM using Adam optimizer, positive activation function(relu), and between 0 and 1 using sigmoid\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, embed_size))\n",
    "model.add(Bidirectional(LSTM(32, return_sequences = True)))\n",
    "model.add(GlobalMaxPool1D())\n",
    "model.add(Dense(20, activation=\"relu\"))\n",
    "model.add(Dropout(0.05))\n",
    "model.add(Dense(1, activation=\"sigmoid\"))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "batch_size = 100\n",
    "epochs = 5\n",
    "model.fit(X_t,y, batch_size=batch_size, epochs=epochs, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Epoch 1:\n",
    "\n",
    "    time: 191s \n",
    "    speed: 299ms/step \n",
    "    loss: 0.4645 \n",
    "    accuracy: 0.7541 \n",
    "    val_loss: 0.4357 \n",
    "    val_accuracy: 0.7618\n",
    "\n",
    "Epoch 2:\n",
    "\n",
    "    time: 146s \n",
    "    speed: 227ms/step \n",
    "    loss: 0.4051 \n",
    "    accuracy: 0.7794 \n",
    "    val_loss: 0.4385 \n",
    "    val_accuracy: 0.7543\n",
    "\n",
    "Epoch 3:\n",
    "\n",
    "    time: 150s \n",
    "    speed: 235ms/step \n",
    "    loss: 0.3785 \n",
    "    accuracy: 0.7990 \n",
    "    val_loss: 0.4480 \n",
    "    val_accuracy: 0.7666\n",
    "\n",
    "Epoch 4:\n",
    "\n",
    "    time: 156s \n",
    "    speed: 244ms/step \n",
    "    loss: 0.3473 \n",
    "    accuracy: 0.8201 \n",
    "    val_loss: 0.4616 \n",
    "    val_accuracy: 0.7623\n",
    "\n",
    "Epoch 5:\n",
    "\n",
    "    time: 171s \n",
    "    speed: 267ms/step \n",
    "    loss: 0.3076 \n",
    "    accuracy: 0.8500 \n",
    "    val_loss: 0.5101 \n",
    "    val_accuracy: 0.7552"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Testing the model\n",
    "list_sentences_test = X_test\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\n",
    "X_te = pad_sequences(list_tokenized_test, maxlen=maxlen)\n",
    "prediction = model.predict(X_te)\n",
    "y_pred = (prediction > 0.5)\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "print('F1-score: {0}'.format(f1_score(y_pred, y_test)))\n",
    "print('Confusion matrix:')\n",
    "confusion_matrix(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "F1-score: \n",
    "\n",
    "       0.5307570241762346\n",
    "\n",
    "Confusion matrix:\n",
    "\n",
    "       [[12130,  2188]\n",
    "        [ 2839,  2843]]\n",
    "\n",
    "Accuracy:\n",
    "\n",
    "       74.86%"
   ]
  }
 ]
}