{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1598365719264",
   "display_name": "Python 3.8.3 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset:\n",
    "\n",
    "    https://data.world/crowdflower/sentiment-analysis-in-text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "# Reading the csv dataset into a pandas dataframe\n",
    "df = pd.read_csv('E:/Internships/TCS-iON/Code/MyCode/Tweets/text_emotion.csv', encoding = 'ISO-8859-1') \n",
    "# Adding a column representing 1 for positive and 0 for negative sentiments\n",
    "df['senti'] = df.apply(lambda x: 1 if (x['sentiment'] == 'enthusiasm' or x['sentiment'] == 'surprise' or x['sentiment'] == 'love' or x['sentiment'] == 'fun' or x['sentiment'] == 'happiness' or x['sentiment'] == 'relief') else 0, axis = 1) \n",
    "# Deleting unnecessary columns\n",
    "df = df.drop(['tweet_id', 'sentiment', 'author'], axis = 1) \n",
    "# Converting the data type to string \n",
    "df['content'] = df[\"content\"].astype(\"str\")\n",
    "# Converting all text to lowercase for use\n",
    "df['content'] = df['content'].str.lower()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table border = '1'>\n",
    "<tr align = 'center'>\n",
    "<td>review</td><td></td>\t<td>senti</td>\n",
    "<hr>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>@tiffanylue i know i was listenin to bad habi...</td><td></td>\t<td>0</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>layin n bed with a headache ughhhh...waitin o...</td><td></td>\t<td>0</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>funeral ceremony...gloomy friday...</td><td></td><td>0</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>wants to hang out with friends soon!</td><td></td>\t<td>1</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>@dannycastillo we want to trade with someone w...</td><td></td>\t<td>0</td>\n",
    "</tr>\n",
    "</table>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i am going to start reading the harry potter series again because that is one awesome story."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from nltk import WordNetLemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "# Initialising the nltk stop_words, stemmer and lemmatizer functions\n",
    "stop_words = set(stopwords.words(\"english\")) \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "# Creating a function for text cleaning\n",
    "def textCleanser(myText):\n",
    "    # Converting each tweet to string\n",
    "    myText = str(myText)\n",
    "    # Removing the name titles and the period symbols after it\n",
    "    myText = re.sub(r'[mdsr]r(s)?\\.', '', myText)\n",
    "    # Removing the '@username' mentions\n",
    "    myText = re.sub(r'@\\w+\\s', '', myText)\n",
    "    # Removing punctuation\n",
    "    myPunct = string.punctuation\n",
    "    punctToSpace = str.maketrans(myPunct, len(myPunct)*' ')   \n",
    "    myText = myText.translate(punctToSpace) \n",
    "    # Removing urls\n",
    "    myText = re.sub(r'((http(s?)?)://?)(www\\.?).+\\.com', '', myText)\n",
    "    myText = re.sub(r'http(s?)', '', myText)\n",
    "    # Removing numbers\n",
    "    myText = re.sub(r'\\d+', '', myText)    \n",
    "    # Removing stopwords\n",
    "    myText = [word for word in myText.split(' ') if not word in stop_words]\n",
    "    myText = [word for word in myText if word != '']\n",
    "    # Lemmatizing the text\n",
    "    myText = [lemmatizer.lemmatize(token) for token in myText]\n",
    "    # Stemming the text\n",
    "    # myText = [stemmer.stem(token) for token in myText]\n",
    "    return myText\n",
    "for i in range(len(df['content'])):\n",
    "    df['content'][i] = textCleanser(df['content'][i])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table border = '1'>\n",
    "<tr align = 'center'>\n",
    "<td>review</td><td></td>\t<td>senti</td>\n",
    "<hr>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>[know, listenin, bad, habit, earlier, started,...</td><td></td>\t<td>0</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>[layin, n, bed, headache, ughhhh, waitin, call]</td><td></td>\t<td>0</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>[funeral, ceremony, gloomy, friday]</td><td></td><td>0</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>[want, hang, friend, soon]</td><td></td>\t<td>1</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>[want, trade, someone, houston, ticket, one]</td><td></td>\t<td>0</td>\n",
    "</tr>\n",
    "</table>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "['going', 'start', 'reading', 'harry', 'potter', 'series', 'one', 'awesome', 'story']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myReviews = []\n",
    "for i in range(len(df['content'])):\n",
    "    for j in df['content'][i]:\n",
    "        if j != 'br' and j != 'http':\n",
    "            myReviews.append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import collections\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np \n",
    "np.random.seed(1234)\n",
    "# Initialising the Count Vectorizer\n",
    "cv = CountVectorizer()\n",
    "myBow = cv.fit_transform(myReviews)\n",
    "wordFrequency = dict(zip(cv.get_feature_names(), np.asarray(myBow.sum(axis = 0)).ravel()))\n",
    "wordCounter = collections.Counter(wordFrequency)\n",
    "# Storing the frequency of appearance of words\n",
    "dfWordCounter = pd.DataFrame(wordCounter.most_common(25), columns = ['word', 'frequency'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the top 25 most frequently occuring words\n",
    "plt.close('all')\n",
    "fig, ax = plt.subplots(figsize = (17, 12))\n",
    "sns.barplot(x = 'word', y = 'frequency', data = dfWordCounter, ax = ax)\n",
    "sns.set_palette('pastel') \n",
    "plt.xlabel('Words')\n",
    "plt.ylabel('Frequency of appearance of words')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"top25words.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense , Input , LSTM , Embedding, Dropout , Activation, GRU, Flatten\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Convolution1D\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "# Splitting the dataframe to training and testing data\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[\"review\"], df['senti'],test_size=0.2, random_state=42)\n",
    "# Initialising the tokenizer\n",
    "max_features = 4000\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(X_train)\n",
    "maxlen = 130\n",
    "X_t = pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "y = y_train\n",
    "embed_size = 128\n",
    "# Initializing a bidirectional sequential LSTM using Adam optimizer, positive activation function(relu), and between 0 and 1 using sigmoid\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, embed_size))\n",
    "model.add(Bidirectional(LSTM(32, return_sequences = True)))\n",
    "model.add(GlobalMaxPool1D())\n",
    "model.add(Dense(20, activation=\"relu\"))\n",
    "model.add(Dropout(0.05))\n",
    "model.add(Dense(1, activation=\"sigmoid\"))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "batch_size = 100\n",
    "epochs = 5\n",
    "model.fit(X_t,y, batch_size=batch_size, epochs=epochs, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Epoch 1:\n",
    "\n",
    "    time: 75s \n",
    "    speed: 292ms/step \n",
    "    loss: 0.5886 \n",
    "    accuracy: 0.6948 \n",
    "    val_loss: 0.5567 \n",
    "    val_accuracy: 0.7227\n",
    "\n",
    "Epoch 2:\n",
    "\n",
    "    time: 59s \n",
    "    speed: 229ms/step \n",
    "    loss: 0.5116 \n",
    "    accuracy: 0.7575 \n",
    "    val_loss: 0.5612 \n",
    "    val_accuracy: 0.7181\n",
    "\n",
    "Epoch 3:\n",
    "\n",
    "    time: 60s \n",
    "    speed: 234ms/step \n",
    "    loss: 0.4794 \n",
    "    accuracy: 0.7789 \n",
    "    val_loss: 0.5857 \n",
    "    val_accuracy: 0.7094\n",
    "\n",
    "Epoch 4:\n",
    "\n",
    "    time: 69s \n",
    "    speed: 269ms/step \n",
    "    loss: 0.4484 \n",
    "    accuracy: 0.7964 \n",
    "    val_loss: 0.5969 \n",
    "    val_accuracy: 0.7023\n",
    "\n",
    "Epoch 5:\n",
    "\n",
    "    time: 69s \n",
    "    speed: 271ms/step \n",
    "    loss: 0.4111 \n",
    "    accuracy: 0.8214 \n",
    "    val_loss: 0.6331 \n",
    "    val_accuracy: 0.7073"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Testing the model\n",
    "list_sentences_test = X_test\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\n",
    "X_te = pad_sequences(list_tokenized_test, maxlen=maxlen)\n",
    "prediction = model.predict(X_te)\n",
    "y_pred = (prediction > 0.5)\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "print('F1-score: {0}'.format(f1_score(y_pred, y_test)))\n",
    "print('Confusion matrix:')\n",
    "confusion_matrix(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "F1-score: \n",
    "\n",
    "       0.5906515580736543\n",
    "\n",
    "Confusion matrix:\n",
    "\n",
    "       [[4020, 1400],\n",
    "        [ 912, 1668]]\n",
    "\n",
    "Accuracy:\n",
    "\n",
    "       71.10%"
   ]
  }
 ]
}