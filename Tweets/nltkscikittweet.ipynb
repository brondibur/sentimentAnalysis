{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1598348467892",
   "display_name": "Python 3.8.3 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset:\n",
    "\n",
    "    https://data.world/crowdflower/sentiment-analysis-in-text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "# Reading the csv dataset into a pandas dataframe\n",
    "df = pd.read_csv('E:/Internships/TCS-iON/Code/MyCode/Tweets/text_emotion.csv', encoding = 'ISO-8859-1') \n",
    "# Adding a column representing 1 for positive and 0 for negative sentiments\n",
    "df['senti'] = df.apply(lambda x: 1 if (x['sentiment'] == 'enthusiasm' or x['sentiment'] == 'surprise' or x['sentiment'] == 'love' or x['sentiment'] == 'fun' or x['sentiment'] == 'happiness' or x['sentiment'] == 'relief') else 0, axis = 1) \n",
    "# Deleting unnecessary columns\n",
    "df = df.drop(['tweet_id', 'sentiment', 'author'], axis = 1) \n",
    "# Converting the data type to string \n",
    "df['content'] = df[\"content\"].astype(\"str\")\n",
    "# Converting all text to lowercase for use\n",
    "df['content'] = df['content'].str.lower()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table border = '1'>\n",
    "<tr align = 'center'>\n",
    "<td>review</td><td></td>\t<td>senti</td>\n",
    "<hr>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>@tiffanylue i know i was listenin to bad habi...</td><td></td>\t<td>0</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>layin n bed with a headache ughhhh...waitin o...</td><td></td>\t<td>0</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>funeral ceremony...gloomy friday...</td><td></td><td>0</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>wants to hang out with friends soon!</td><td></td>\t<td>1</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>@dannycastillo we want to trade with someone w...</td><td></td>\t<td>0</td>\n",
    "</tr>\n",
    "</table>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i am going to start reading the harry potter series again because that is one awesome story."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from nltk import WordNetLemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "# Initialising the nltk stop_words, stemmer and lemmatizer functions\n",
    "stop_words = set(stopwords.words(\"english\")) \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "# Creating a function for text cleaning\n",
    "def textCleanser(myText):\n",
    "    # Converting each tweet to string\n",
    "    myText = str(myText)\n",
    "    # Removing the name titles and the period symbols after it\n",
    "    myText = re.sub(r'[mdsr]r(s)?\\.', '', myText)\n",
    "    # Removing the '@username' mentions\n",
    "    myText = re.sub(r'@\\w+\\s', '', myText)\n",
    "    # Removing punctuation\n",
    "    myPunct = string.punctuation\n",
    "    punctToSpace = str.maketrans(myPunct, len(myPunct)*' ')   \n",
    "    myText = myText.translate(punctToSpace) \n",
    "    # Removing urls\n",
    "    myText = re.sub(r'((http(s?)?)://?)(www\\.?).+\\.com', '', myText)\n",
    "    myText = re.sub(r'http(s?))', '', myText)\n",
    "    # Removing numbers\n",
    "    myText = re.sub(r'\\d+', '', myText)    \n",
    "    # Removing stopwords\n",
    "    myText = [word for word in myText.split(' ') if not word in stop_words]\n",
    "    myText = [word for word in myText if word != '']\n",
    "    # Lemmatizing the text\n",
    "    myText = [lemmatizer.lemmatize(token) for token in myText]\n",
    "    # Stemming the text\n",
    "    # myText = [stemmer.stem(token) for token in myText]\n",
    "    return myText\n",
    "for i in range(len(df['content'])):\n",
    "    df['content'][i] = textCleanser(df['content'][i])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table border = '1'>\n",
    "<tr align = 'center'>\n",
    "<td>review</td><td></td>\t<td>senti</td>\n",
    "<hr>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>[know, listenin, bad, habit, earlier, started,...</td><td></td>\t<td>0</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>[layin, n, bed, headache, ughhhh, waitin, call]</td><td></td>\t<td>0</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>[funeral, ceremony, gloomy, friday]</td><td></td><td>0</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>[want, hang, friend, soon]</td><td></td>\t<td>1</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>[want, trade, someone, houston, ticket, one]</td><td></td>\t<td>0</td>\n",
    "</tr>\n",
    "</table>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "['going', 'start', 'reading', 'harry', 'potter', 'series', 'one', 'awesome', 'story']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myReviews = []\n",
    "for i in range(len(df['content'])):\n",
    "    for j in df['content'][i]:\n",
    "        if j != 'br' and j != 'http':\n",
    "            myReviews.append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import collections\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np \n",
    "np.random.seed(1234)\n",
    "# Initialising the Count Vectorizer\n",
    "cv = CountVectorizer()\n",
    "myBow = cv.fit_transform(myReviews)\n",
    "wordFrequency = dict(zip(cv.get_feature_names(), np.asarray(myBow.sum(axis = 0)).ravel()))\n",
    "wordCounter = collections.Counter(wordFrequency)\n",
    "# Storing the frequency of appearance of words\n",
    "dfWordCounter = pd.DataFrame(wordCounter.most_common(25), columns = ['word', 'frequency'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the top 25 most frequently occuring words\n",
    "plt.close('all')\n",
    "fig, ax = plt.subplots(figsize = (17, 12))\n",
    "sns.barplot(x = 'word', y = 'frequency', data = dfWordCounter, ax = ax)\n",
    "sns.set_palette('pastel') \n",
    "plt.xlabel('Words')\n",
    "plt.ylabel('Frequency of appearance of words')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"top25words.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Defining a dummy function for the tokenizer and preprocessor inputs of the vectorizers so that we can use our tokenized words as input\n",
    "def dummy_fun(doc):\n",
    "    return doc\n",
    "tfidf = TfidfVectorizer(sublinear_tf=True, max_df = 2000, min_df = 50, norm='l2', encoding='latin-1', ngram_range=(1, 2), stop_words='english', tokenizer = dummy_fun, preprocessor = dummy_fun)\n",
    "features = []\n",
    "features = tfidf.fit_transform(df.content).toarray()\n",
    "labels = df.senti\n",
    "print(features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features Shape:\n",
    "\n",
    "    (40000, 823)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "# Splitting the dataframe to training and testing datasets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[\"review\"], df['senti'],test_size=0.2, random_state=42)\n",
    "vectorizerCount = CountVectorizer(tokenizer = dummy_fun, preprocessor = dummy_fun)\n",
    "X_train_counts = vectorizerCount.fit_transform(X_train)\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "nBayes = MultinomialNB().fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_= nBayes.predict(vectorizerCount.transform(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Testing the data\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(y_test, y_,normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy:\n",
    "\n",
    "    71.31%"
   ]
  }
 ]
}